import { useRef, useEffect, useCallback } from 'react';

// Singleton AudioContext
let globalAudioContext: AudioContext | null = null;

const getAudioContext = (): AudioContext => {
  if (!globalAudioContext || globalAudioContext.state === 'closed') {
    const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
    globalAudioContext = new AudioContextClass();
  }
  return globalAudioContext;
};

// iOS éŸ³é¢‘æ¿€æ´»ï¼šå¿…é¡»åœ¨ç”¨æˆ·äº¤äº’æ—¶åŒæ­¥è°ƒç”¨
let audioActivated = false;

const activateAudioOnUserInteraction = () => {
  if (audioActivated) return;
  
  const ctx = getAudioContext();
  
  // å°è¯•æ¢å¤ AudioContext
  if (ctx.state === 'suspended') {
    ctx.resume().then(() => {
      console.log('[Audio] AudioContext resumed successfully');
      audioActivated = true;
    }).catch(err => {
      console.warn('[Audio] Failed to resume AudioContext:', err);
    });
  } else {
    audioActivated = true;
  }
  
  // æ’­æ”¾é™éŸ³éŸ³é¢‘æ¥"è§£é”" iOS éŸ³é¢‘
  const silentBuffer = ctx.createBuffer(1, 1, 22050);
  const source = ctx.createBufferSource();
  source.buffer = silentBuffer;
  source.connect(ctx.destination);
  source.start(0);
  
  console.log('[Audio] Audio activation triggered');
};

// åœ¨é¡µé¢é¦–æ¬¡ç”¨æˆ·äº¤äº’æ—¶æ¿€æ´»éŸ³é¢‘
if (typeof window !== 'undefined') {
  const activationEvents = ['touchstart', 'touchend', 'mousedown', 'click', 'keydown'];
  
  const handleFirstInteraction = () => {
    activateAudioOnUserInteraction();
    // ç§»é™¤ç›‘å¬å™¨ï¼Œåªéœ€æ¿€æ´»ä¸€æ¬¡
    activationEvents.forEach(event => {
      document.removeEventListener(event, handleFirstInteraction, true);
    });
  };
  
  activationEvents.forEach(event => {
    document.addEventListener(event, handleFirstInteraction, true);
  });
}

// å¯ç”¨ä¹å™¨åˆ—è¡¨
export const INSTRUMENTS = {
  acoustic_grand_piano: { name: 'é’¢ç´', icon: 'ğŸ¹', category: 'é”®ç›˜' },
  acoustic_guitar_nylon: { name: 'å¤å…¸å‰ä»–', icon: 'ğŸ¸', category: 'å¼¦ä¹' },
  acoustic_guitar_steel: { name: 'æ°‘è°£å‰ä»–', icon: 'ğŸ¸', category: 'å¼¦ä¹' },
  electric_guitar_clean: { name: 'ç”µå‰ä»–', icon: 'ğŸ¸', category: 'å¼¦ä¹' },
  violin: { name: 'å°æç´', icon: 'ğŸ»', category: 'å¼¦ä¹' },
  cello: { name: 'å¤§æç´', icon: 'ğŸ»', category: 'å¼¦ä¹' },
  flute: { name: 'é•¿ç¬›', icon: 'ğŸµ', category: 'ç®¡ä¹' },
  clarinet: { name: 'å•ç°§ç®¡', icon: 'ğŸµ', category: 'ç®¡ä¹' },
  soprano_sax: { name: 'è¨å…‹æ–¯', icon: 'ğŸ·', category: 'ç®¡ä¹' },
  trumpet: { name: 'å°å·', icon: 'ğŸº', category: 'é“œç®¡' },
  trombone: { name: 'é•¿å·', icon: 'ğŸº', category: 'é“œç®¡' },
  vibraphone: { name: 'é¢¤éŸ³ç´', icon: 'ğŸ¶', category: 'æ‰“å‡»' },
  marimba: { name: 'é©¬æ—å·´', icon: 'ğŸ¥', category: 'æ‰“å‡»' },
  xylophone: { name: 'æœ¨ç´', icon: 'ğŸ¶', category: 'æ‰“å‡»' },
  music_box: { name: 'éŸ³ä¹ç›’', icon: 'ğŸµ', category: 'ç‰¹æ®Š' },
  celesta: { name: 'é’¢ç‰‡ç´', icon: 'ğŸ””', category: 'ç‰¹æ®Š' },
} as const;

export type InstrumentId = keyof typeof INSTRUMENTS;

// é»˜è®¤ä¹å™¨
let currentInstrument: InstrumentId = 'acoustic_grand_piano';

// è·å–å½“å‰ä¹å™¨
export const getCurrentInstrument = () => currentInstrument;

// è®¾ç½®å½“å‰ä¹å™¨
export const setCurrentInstrument = (instrument: InstrumentId) => {
  currentInstrument = instrument;
};

// Cache for loaded audio buffers - æŒ‰ä¹å™¨åˆ†ç±»
const audioBufferCache: Map<string, AudioBuffer> = new Map();

// Note names for MIDI to note conversion
const NOTE_NAMES = ['C', 'Db', 'D', 'Eb', 'E', 'F', 'Gb', 'G', 'Ab', 'A', 'Bb', 'B'];

// Convert MIDI number to note name for the soundfont URL
const midiToNoteName = (midi: number): string => {
  const noteIndex = midi % 12;
  const octave = Math.floor(midi / 12) - 1;
  return `${NOTE_NAMES[noteIndex]}${octave}`;
};

// Soundfont URL base (FluidR3_GM supports multiple instruments)
const SOUNDFONT_BASE = 'https://gleitz.github.io/midi-js-soundfonts/FluidR3_GM/';

// Load a sample for a given MIDI note and instrument
const loadInstrumentSample = async (midi: number, instrument: InstrumentId = currentInstrument): Promise<AudioBuffer | null> => {
  const cacheKey = `${instrument}:${midi}`;
  
  // Check cache first
  if (audioBufferCache.has(cacheKey)) {
    return audioBufferCache.get(cacheKey)!;
  }

  // Clamp to valid range
  const clampedMidi = Math.max(21, Math.min(108, midi)); // A0 to C8
  const noteName = midiToNoteName(clampedMidi);
  const url = `${SOUNDFONT_BASE}${instrument}-mp3/${noteName}.mp3`;

  try {
    const ctx = getAudioContext();
    const response = await fetch(url);
    if (!response.ok) {
      console.warn(`Failed to load ${instrument} sample for ${noteName}`);
      return null;
    }
    const arrayBuffer = await response.arrayBuffer();
    const audioBuffer = await ctx.decodeAudioData(arrayBuffer);
    
    // Cache the loaded buffer
    audioBufferCache.set(cacheKey, audioBuffer);
    return audioBuffer;
  } catch (error) {
    console.warn(`Error loading ${instrument} sample for ${noteName}:`, error);
    return null;
  }
};

// Preload common notes for an instrument
const preloadCommonNotes = async (instrument: InstrumentId = 'acoustic_grand_piano') => {
  // Preload C3 to C6 (most commonly used range)
  const notesToPreload = [];
  for (let midi = 48; midi <= 84; midi++) {
    notesToPreload.push(loadInstrumentSample(midi, instrument));
  }
  await Promise.all(notesToPreload);
};

// Preload function for switching instruments
export const preloadInstrument = async (instrument: InstrumentId) => {
  await preloadCommonNotes(instrument);
};

// Start preloading default piano when module loads
preloadCommonNotes('acoustic_grand_piano');

interface ActiveNote {
  source: AudioBufferSourceNode;
  gainNode: GainNode;
}

export const useAudioPlayer = () => {
  const currentNoteRef = useRef<ActiveNote | null>(null);
  
  // Always ready after initial module load (preloading is async but non-blocking)
  const isReady = true;

  const stopCurrentNote = useCallback(() => {
    if (currentNoteRef.current) {
      const { source, gainNode } = currentNoteRef.current;
      try {
        // Fade out quickly to avoid clicks
        const ctx = getAudioContext();
        gainNode.gain.setValueAtTime(gainNode.gain.value, ctx.currentTime);
        gainNode.gain.exponentialRampToValueAtTime(0.001, ctx.currentTime + 0.05);
        setTimeout(() => {
          try {
            source.stop();
            source.disconnect();
            gainNode.disconnect();
          } catch (e) {
            // Ignore
          }
        }, 60);
      } catch (e) {
        // Ignore errors
      }
      currentNoteRef.current = null;
    }
  }, []);

  const playNote = useCallback(async (frequency: number, duration: number = 1.5, _type?: OscillatorType, instrument?: InstrumentId) => {
    // Stop any currently playing note first
    stopCurrentNote();

    const ctx = getAudioContext();
    
    // Resume if suspended (iOS requirement)
    if (ctx.state === 'suspended') {
      try {
        await ctx.resume();
        console.log('[Audio] Context resumed in playNote');
      } catch (err) {
        console.warn('[Audio] Failed to resume context:', err);
      }
    }
    
    // é¢å¤–çš„ iOS æ¿€æ´»å°è¯•
    if (!audioActivated) {
      activateAudioOnUserInteraction();
    }

    // Convert frequency to MIDI note number
    const midi = Math.round(12 * Math.log2(frequency / 440) + 69);
    
    // Try to load the instrument sample (use passed instrument or current global setting)
    const buffer = await loadInstrumentSample(midi, instrument || currentInstrument);
    
    if (!buffer) {
      // Fallback to simple sine wave if sample fails to load
      const osc = ctx.createOscillator();
      const gain = ctx.createGain();
      osc.type = 'sine';
      osc.frequency.setValueAtTime(frequency, ctx.currentTime);
      gain.gain.setValueAtTime(1.5, ctx.currentTime);
      gain.gain.exponentialRampToValueAtTime(0.001, ctx.currentTime + duration);
      osc.connect(gain);
      gain.connect(ctx.destination);
      osc.start();
      osc.stop(ctx.currentTime + duration);
      return;
    }

    // Play the sampled note
    const source = ctx.createBufferSource();
    const gainNode = ctx.createGain();
    
    source.buffer = buffer;
    source.connect(gainNode);
    gainNode.connect(ctx.destination);
    
    // Set initial volume (increased for better audibility)
    const volume = 2.0;
    gainNode.gain.setValueAtTime(volume, ctx.currentTime);
    
    // Natural decay - let the sample play, then fade out
    const fadeOutStart = ctx.currentTime + duration - 0.3;
    gainNode.gain.setValueAtTime(volume, fadeOutStart);
    gainNode.gain.exponentialRampToValueAtTime(0.001, ctx.currentTime + duration);
    
    source.start(0);
    
    // Store reference
    currentNoteRef.current = { source, gainNode };
    
    // Stop after duration
    source.stop(ctx.currentTime + duration + 0.1);
    
    // Cleanup
    source.onended = () => {
      try {
        source.disconnect();
        gainNode.disconnect();
      } catch (e) {
        // Ignore
      }
      if (currentNoteRef.current?.source === source) {
        currentNoteRef.current = null;
      }
    };
  }, [stopCurrentNote]);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      stopCurrentNote();
    };
  }, [stopCurrentNote]);

  return { playNote, stopCurrentNote, isReady };
};
